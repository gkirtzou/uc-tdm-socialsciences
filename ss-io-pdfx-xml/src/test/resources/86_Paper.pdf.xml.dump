======== CAS 0 begin ==================================

-------- View _InitialView begin ----------------------------------

DocumentMetaData
   sofa: _InitialView
   begin: 0
   end: 18784
   language: "en"
   documentTitle: "The Denoised Web Treebank: Evaluating Dependency Parsing under Noisy Input Conditions"
   documentId: "The Denoised Web Treebank: Evaluating Dependency Parsing under Noisy Input Conditions"
   isLastSegment: false

CAS-Text:
The Denoised Web Treebank: Evaluating Dependency Parsing under Noisy Input ConditionsILLC University of Amsterdam, CLCG University of Groningen J.Daiber@uva.nl, R.van.der.Goot@rug.nl Abstract We introduce the Denoised Web Treebank: a treebank including a normalization layer and a corresponding evaluation metric for dependency parsing of noisy text, such as Tweets. This benchmark enables the evaluation of parser robustness as well as text normalization methods, including normalization as machine translation and unsupervised lexical normalization, directly on syntactic trees. Experiments show that text normalization together with a combination of domain-specific and generic part-of-speech taggers can lead to a significant improvement in parsing accuracy on this test set. Keywords: Parsing, Part-of-Speech Tagging, Social Media Processing, Web TreebankThe quality of automatic syntactic analysis of clean, in- domain text has improved steadily in recent decades. Out- of-domain text and grammatically noisy text, on the other hand, remain an obstacle and often lead to significant de- creases in parsing accuracy. Recently, a lot of effort has been put into adapting natural language processing tools, such as named entity recognition (Liu et al., 2012) and POS tagging (Gimpel et al., 2011), to noisy content. In this paper, we focus on dependency parsing of noisy text. Specif- ically, we are interested in how much parse quality can be gained by text normalization. For this, we introduce a new dependency treebank with a normalization layer. This new dataset can be used to quantify the influence of text normalization on the parsing of user-generated content. The contributions of this paper are as follows: (1) We introduce the Denoised Web Treebank, a new Twitter dependency treebank with a normalization layer; (2) we propose a corresponding noise-aware evaluation metric; and (3) we use this dataset and the metric as a benchmark to evaluate the impact of text normalization on dependency parsing of user-generated content.For the domain of web data, various datasets and treebanks have been introduced. Table 1 provides an overview of all relevant English treebanks. The constituency treebanks mentioned here were created using the English Web Treebank annotation guidelines (Bies et al., 2012), which are an addendum to the Penn Treebank guidelines (Bies et al., 1995). These guidelines discuss domain-specific phenomena, including adaptations of existing labels as well as the addition of new labels for novel linguistic constructions. Foster et al. (2011a) de- scribe a constituency treebank consisting of two domains; Twitter and sports forums. The Twitter part is of compa- rable size to our treebank and is described in more detail in Foster et al. (2011b). The dependency treebanks show greater diversity in annotation. The English Web Treebank (Silveira et al., 2014) is annotated using the Universal Dependencies guidelines with additional relation types for the web domain. A very different approach is taken for the annotation of the Tweebank (Kong et al., 2014). In its format, individual words can be skipped in the annotation. This is motivated by the idea that not all words in a Tweet contribute significantly to the syntactic structure and their inclusion would lead to arbitrary decisions unhelpful for most downstream applications. Additionally, because Tweets are used as units instead of sentences, having multiple roots is allowed. This adjusted dependency format makes it harder to use existing parsers with this dataset. The Foreebank (Kaljahi et al., 2015), a treebank focusing on forum text, is the only other treebank that includes normalization annotation. It includes manual normalizations of the raw text, and constituency trees of the normalized sentences. The normalization is kept as minimal as possible and is represented in the tree by appending an error suffix to the POS tags. The Foreebank allows analysis of the effect of different errors on the parsing performance of a constituency parser. Our contribution, the Denoised Web Treebank, fills the gap of a native (i.e., non-converted) dependency treebank including normalizations for the web domain. In the past, automatic conversions were used for this task (Petrov and McDonald, 2012; Foster et al., 2011a) using the Stanford Converter (De Marneffe et al., 2006). But for the noisy web domain, the conversions might be of questionable quality. Previous work on the parsing of web data has mostly focused on complementing existing parsers with semi- supervised data The amount of training data can be artifi- cially enlarged by using self-training or up-training (Petrov and McDonald, 2012; Foster et al., 2011b). Another source of semi-supervised improvements can be gained from using features gathered from large amounts of unannotated texts (Kong et al., 2014). A completely different approach is taken by Khan et al. (2013), where the most appropri- ate training trees are found in the train treebank for each sentence.The goal of the normalization was to leave the original tokens intact and not to replace them by their normalized forms directly. Hence, we keep both the original tokens and the normalized version of the sentences with word alignments. Figure 1 depicts a gold standard dependency graph including the alignments to the original tokens. Abbreviations Abbreviations and slang expressions are expanded whenever necessary for syntactic reasons. Exam- ples include instances such as “cu”, used as the short form of see and you, which as a single token would include both the verb and the object of the sentence. Punctuation Punctuation is inserted if it is necessary to disambiguate the sentence meaning. Emoticons, such as :), are kept intact. Zero copulas The data contains several cases of zero copula, i.e. a copula verb is not realized in the sentence. These occurrences are annotated by inserting the copula verb in the normalized version of the sentence (see Figure 1).The normalized tokens were automatically parsed using a generative phrase structure parser 2 and then converted to dependencies. Both part-of-speech tags and dependency annotations were then manually corrected in two passes. The dependency annotations follow the format of the CoNLL-X shared task on multilingual dependency parsing (Buchholz and Marsi, 2006), with the following adaptations: Emoticons Emoticons are kept intact, tagged with the part-of-speech tag UH (interjection) and are attached to the head of the sentence. Domain-specific elements Twitter-specific syntax was treated as follows: Usernames at the start of a sentence that do not fulfill a syntactic role, e.g. as the subject, are attached to the main verb using the DEP (unclassified) dependency relation. In all other cases, usernames are treated as proper nouns. RT and similar markers, as well as hashtags indicat- ing the topic of a Tweet (e.g. #worldcup) are attached to the main verb as DEP.1 English Aspell dictionary: http://aspell.net/ 2 https://code.google.com/p/ berkeleyparser/P ROOT COORD P OBJ NMOD SBJ NMOD OBJ You . are . the . little . . guy . , . are . you . ? . U ∅ da lil guy , ru ?We collected all Tweets within a period of 24 hours from January 07, 2012 00:00 until 23:59 GMT. To avoid possible biases of automatic language identification tools towards well-formed language, we manually classified the Tweets in random order into English and non-English Tweets until we reached a reasonably-sized corpus of Tweets classified as English. We then manually split this corpus into sentences and randomly selected 250 sentences as a development set and 250 sentences as a test set. Table 1 compares some basic statistics of this treebank against other Web treebanks. Out-of-vocabulary rate is calculated against the English dictionary of the GNU Aspell spell checker. 1Our dataset provides alignments between the gold standard and the original tokens, allowing for insertions, deletions and modifications. Hence, the standard dependency parsing metrics, unlabeled and labeled attachment scores, are no longer sufficient. In our dataset, there may not be a di- rect one-to-one correspondence between the predicted tree and the gold tree. Hence, we allow the parser to make any insertions, deletions and modifications to the tokens under the assumption that it provides an alignment between the modified tokens and the original tokens. The evaluation is then performed using a metric based on precision and recall values calculated using these alignments. Aligned precision and recall Based on the normalized side of the gold standard and the parser’s aligned predictions, we calculate precision, recall and F 1 score for dependencies (Eq. 1–3). We base the evaluation metric on the standard definitions of precision and recall, which are widely used in natural language processing. In Eq. 1 and 2, TP , FP and FN are the numbers of true positive, false positive, and false negative results. The F 1 measure is the harmonic mean of precision and recall.TP precision = (1) TP + FP TP recall = (2) TP + FN precision · recall F 1 = 2 · (3) precision + recallDefinition Formally, when comparing the parser output against the dataset, the following information is provided for each instance: − the original sentence S O − a predicted dependency tree D P = ⟨V P , E P ⟩ − a gold dependency tree D G = ⟨V G , E G ⟩ − alignment function a P for predicted tokens − alignment function a G for gold tokens For each parsed dependency tree, S O is the sequence of original, non-normalized tokens. The two alignment functions a G and a P map the gold tokens and the predicted tokens to the original tokens in S O . In the case of an insertion, the new token cannot be aligned to any of the original tokens in S O and, therefore, such insertions are mapped to an artificial NULL token. Unlabeled dependencies Based on all test instances, we calculate the total number of true positive, false positive and false negative dependency relations as follows: For each gold dependency tree D G = ⟨V G , E G ⟩ and each predicted dependency tree D P = ⟨V P , E P ⟩, let M G and M P be the set of dependency relations mapped to the original tokens in S O : M G = {⟨a G (w i ), a G (w j )⟩ | ⟨w i , r, w j ⟩ ∈ E G } M P = {⟨a P (w i ), a P (w j )⟩ | ⟨w i , r, w j ⟩ ∈ E P } The true positive, false positive and false negative dependency relations can then be calculated as: ∑ TP = |M G ∩ M P | ⟨S O ,D P ,D G ,a P ,a G ⟩ ∑ FP = |M P \ M G | ⟨S O ,D P ,D G ,a P ,a G ⟩ ∑ FN = |M G \ M P | ⟨S O ,D P ,D G ,a P ,a G ⟩ Labeled dependencies To measure labeled dependencies, the dependency type is added to the head-modifier pair in M P and M G : M G ′ = {⟨a G (w i ), r, a G (w j )⟩ | ⟨w i , r, w j ⟩ ∈ E G } M P ′ = {⟨a P (w i ), r, a P (w j )⟩ | ⟨w i , r, w j ⟩ ∈ E P } Relation to other metrics This metric can be seen as a generalization of the commonly used attachment score measure. If there is a one-to-one alignment between every predicted token and every gold token, the unlabeled and labeled aligned F 1 scores are equivalent to the unlabeled (UAS) and labeled attachment score (LAS).Having introduced our dataset and the corresponding evaluation metric, we can evaluate the impact of two methods commonly used to aid in the parsing of noisy content: noise-robust part-of-speech tagging and text normalization.POS tagging is a necessary preprocessing step for many parsing algorithms. Previous studies (e.g., Foster et al. (2011b)) have shown that the accuracy of POS tagging can suffer significantly from noisy content. However, it is possible to adapt POS taggers to this type of input. In this ex- periment, we will briefly introduce approaches to adapting POS taggers and perform an evaluation on our dataset. Domain-specific tagging Gimpel et al. (2011) present a domain-specific conditional random field POS tagger using a coarse part-of-speech tagset of 25 tags that was specifically designed for and trained on Twitter data. The tagset includes tags for social media-specific tokens, such as URLs, email addresses, emoticons, Twitter hashtags and usernames. Role of POS tags in the parser For our experiments, we use the discriminative graph-based maximum spanning tree (MST) parser (McDonald et al., 2005). This dependency parser expects both fine- and coarse-grained tags as features in its well-established standard setting. Since we are interested in the influence of POS tagging on parse quality instead of the impact of individual features in the parser, we use this standard setting but combine the coarse- grained tags from the domain-specific tagger with the POS tags produced by a POS tagger with a less coarse-grained tagset. Both are combined by first determining n-best fine- grained tags for each token. For hidden Markov models, the probability of the tags occurring at a given position can be calculated using the forward-backward algorithm as P (t i = t) = α i (t)β i (t), where α i (t) is the total probability of all possible tag sequences ending in the tag t at the ith token and β i (t) is the total probability of all tag sequences starting from tag t at the ith token and contin- uing to the end of the sentence (Jurafsky and Martin, 2000; Prins, 2005). The n-best fine-grained tags are then combined with the coarse tags by a simple voting rule. Our experiments use a standard trigram HMM tagger 3 (Brants, 2000) and the OpenNLP maximum entropy tagger. 4 Impact on parse quality Table 2 shows the influence of POS tagging on the performance of the MST parser on the development part of our dataset. Statistical significance testing is performed using bootstrap resampling (Efron and Tibshirani, 1993). Except for the last row of the table, all tagging is performed without any text normalization. The last row demonstrates the upper bound performance on this task, by using both gold text normalization and gold part-of-speech tags. These results show that combining a generic part-of-speech tagger with a more coarse-grained domain-specific tagger can lead to measurable improvements in parse quality.3 https://github.com/danieldk/jitar 4 http://opennlp.apache.org/After considering the influence of the underlying POS tagger on parse quality, we now turn to the question of how much the parsing of noisy content is influenced by text normalization. For this, we evaluate two common text normalization methods: unsupervised normalization via lexical replacements and normalization based on machine translation. Unsupervised lexical normalization Various unsupervised methods for text normalization have been suggested in the relevant literature. A popular approach is to perform lexical normalization by correcting individual tokens. We implement the model for lexical normalization of text messages by Han and Baldwin (2011). This method works in analogy to spell checking, with the biggest difference that in short message data ill-formedness is often intentional, for example due to the message size limit. The model performs normalization only on the token level. Normalization as machine translation Research in short message normalization has shown that another effec- tive method is to treat the task as a machine translation problem. Aw et al. (2006) and Raghunathan and Krawczyk (2009) explore phrase-based statistical machine translation as a preprocessing step for various NLP tasks involving text messages. As part of this effort, they manually normalize a set of 5.000 and 2.500 messages respectively. While these corpora are not created for social media services such as Twitter, they nonetheless provide reasonable training corpora for our experiments as the restrictions of both domains are similar. Based on this corpus, we train a standard Moses baseline system 5 (Koehn et al., 2007) using GIZA++ for word alignments and the grow-diag-final symmetrization heuristic. An n-gram language model is built on the English side of the news-commentary data set using IRSTLM (Federico and Cettolo, 2007). Model weights are estimated using MERT (Och, 2003). All experiments are performed on the development part of our dataset. Twitter-specific processing In order to isolate the influence of the text normalization, Twitter-specific syntax is parsed using a set of deterministic rules. Tokens such as retweet indicators and usernames at the start of a Tweet and URLs and hash tags at the end of a Tweet are removed from the text and pushed onto a stack. The remaining text is then parsed using the underlying dependency parser and the Twitter-specific tokens are re-attached to the tree accord- ing to a fixed set of rules. This deterministic handling of Twitter-specific syntax is applied to all further experiments in Table 3. Impact on parse quality Table 3 presents the results of the text normalization schemes on the development part of our dataset. The results show that a combination of lexical and MT-based normalization approaches leads to results close to the upper bound set by gold standard normalization. Although the machine translation system was trained on a different domain, its application leads to better parsing results. This improved performance is most likely due to the fact that the method is able to normalize sequences of words on the phrase level instead of being restricted to single-word replacements.5 http://statmt.org/moses/?n=Moses. Baseline* statistically significant against non-normalized baseline at p-value < 0.05.User-generated content on the web constitutes a rich and important source of information for many use cases. However, parsing of such noisy data still poses challenges for many parsing algorithms. In this paper, we have compared various strategies for adapting dependency parsing to noisy input conditions. In order to do so, we introduced a noise- aware benchmark for dependency parsing consisting of a treebank and a corresponding evaluation metric. Our experiments on this new dataset show that text normalization improves parse quality significantly, especially if the normalization method can go beyond the word level (e.g. using machine translation). To encourage future progress in this area, we make available both the Denoised Web Treebank and the newly introduced noise-aware evaluation metric. 7We thank Gertjan van Noord for his valuable feedback. Parts of this work were supported through the Erasmus Mundus European Masters Program in Language and Com- munication Technologies (EM-LCT). The first author is supported by the EXPERT (EXPloiting Empirical ap- pRoaches to Translation) Initial Training Network (ITN) of the European Union’s Seventh Framework Programme. The second author is supported by the Nuance Foundation.6 Tags predicted by coarse + n-best MaxEnt. 7 http://jodaiber.de/DenoisedWebTreebank
[The Denoised Web Treebank: Evaluating Dependency Parsing under Noisy Input Conditions]
Paragraph
   sofa: _InitialView
   begin: 0
   end: 85
[The Denoised Web Treebank: Evaluating Dependency Parsing under Noisy Input Conditions]
Sentence
   sofa: _InitialView
   begin: 0
   end: 85
[ILLC University of Amsterdam, CLCG University of Groningen J.Daiber@uva.nl, R.van.der.Goot@rug.nl Abstract We introduce the Denoised Web Treebank: a treebank including a normalization layer and a corresponding evaluation metric for dependency parsing of noisy text, such as Tweets. This benchmark enables the evaluation of parser robustness as well as text normalization methods, including normalization as machine translation and unsupervised lexical normalization, directly on syntactic trees. Experiments show that text normalization together with a combination of domain-specific and generic part-of-speech taggers can lead to a significant improvement in parsing accuracy on this test set. Keywords: Parsing, Part-of-Speech Tagging, Social Media Processing, Web Treebank]
Paragraph
   sofa: _InitialView
   begin: 85
   end: 860
[ILLC University of Amsterdam, CLCG University of Groningen J.Daiber@uva.nl, R.van.der.Goot@rug.nl Abstract We introduce the Denoised Web Treebank: a treebank including a normalization layer and a corresponding evaluation metric for dependency parsing of noisy text, such as Tweets.]
Sentence
   sofa: _InitialView
   begin: 85
   end: 366
[ This benchmark enables the evaluation of parser robustness as well as text normalization methods, including normalization as machine translation and unsupervised lexical normalization, directly on syntactic trees.]
Sentence
   sofa: _InitialView
   begin: 366
   end: 580
[ Experiments show that text normalization together with a combination of domain-specific and generic part-of-speech taggers can lead to a significant improvement in parsing accuracy on this test set.]
Sentence
   sofa: _InitialView
   begin: 580
   end: 779
[ Keywords: Parsing, Part-of-Speech Tagging, Social Media Processing, Web Treebank]
Sentence
   sofa: _InitialView
   begin: 779
   end: 860
[The quality of automatic syntactic analysis of clean, in- domain text has improved steadily in recent decades. Out- of-domain text and grammatically noisy text, on the other hand, remain an obstacle and often lead to significant de- creases in parsing accuracy. Recently, a lot of effort has been put into adapting natural language processing tools, such as named entity recognition (Liu et al., 2012) and POS tagging (Gimpel et al., 2011), to noisy content. In this paper, we focus on dependency parsing of noisy text. Specif- ically, we are interested in how much parse quality can be gained by text normalization. For this, we introduce a new dependency treebank with a normalization layer. This new dataset can be used to quantify the influence of text normalization on the parsing of user-generated content. The contributions of this paper are as follows: (1) We introduce the Denoised Web Treebank, a new Twitter dependency treebank with a normalization layer; (2) we propose a corresponding noise-aware evaluation metric; and (3) we use this dataset and the metric as a benchmark to evaluate the impact of text normalization on dependency parsing of user-generated content.]
Paragraph
   sofa: _InitialView
   begin: 860
   end: 2040
[The quality of automatic syntactic analysis of clean, in- domain text has improved steadily in recent decades.]
Sentence
   sofa: _InitialView
   begin: 860
   end: 970
[ Out- of-domain text and grammatically noisy text, on the other hand, remain an obstacle and often lead to significant de- creases in parsing accuracy.]
Sentence
   sofa: _InitialView
   begin: 970
   end: 1121
[ Recently, a lot of effort has been put into adapting natural language processing tools, such as named entity recognition (Liu et al., 2012) and POS tagging (Gimpel et al., 2011), to noisy content.]
Sentence
   sofa: _InitialView
   begin: 1121
   end: 1318
[ In this paper, we focus on dependency parsing of noisy text.]
Sentence
   sofa: _InitialView
   begin: 1318
   end: 1379
[ Specif- ically, we are interested in how much parse quality can be gained by text normalization.]
Sentence
   sofa: _InitialView
   begin: 1379
   end: 1476
[ For this, we introduce a new dependency treebank with a normalization layer.]
Sentence
   sofa: _InitialView
   begin: 1476
   end: 1553
[ This new dataset can be used to quantify the influence of text normalization on the parsing of user-generated content.]
Sentence
   sofa: _InitialView
   begin: 1553
   end: 1672
[ The contributions of this paper are as follows: (1) We introduce the Denoised Web Treebank, a new Twitter dependency treebank with a normalization layer; (2) we propose a corresponding noise-aware evaluation metric; and (3) we use this dataset and the metric as a benchmark to evaluate the impact of text normalization on dependency parsing of user-generated content.]
Sentence
   sofa: _InitialView
   begin: 1672
   end: 2040
[For the domain of web data, various datasets and treebanks have been introduced. Table 1 provides an overview of all relevant English treebanks. The constituency treebanks mentioned here were created using the English Web Treebank annotation guidelines (Bies et al., 2012), which are an addendum to the Penn Treebank guidelines (Bies et al., 1995). These guidelines discuss domain-specific phenomena, including adaptations of existing labels as well as the addition of new labels for novel linguistic constructions. Foster et al. (2011a) de- scribe a constituency treebank consisting of two domains; Twitter and sports forums. The Twitter part is of compa- rable size to our treebank and is described in more detail in Foster et al. (2011b). The dependency treebanks show greater diversity in annotation. The English Web Treebank (Silveira et al., 2014) is annotated using the Universal Dependencies guidelines with additional relation types for the web domain. A very]
Paragraph
   sofa: _InitialView
   begin: 2040
   end: 3008
[For the domain of web data, various datasets and treebanks have been introduced.]
Sentence
   sofa: _InitialView
   begin: 2040
   end: 2120
[ Table 1 provides an overview of all relevant English treebanks.]
Sentence
   sofa: _InitialView
   begin: 2120
   end: 2184
[ The constituency treebanks mentioned here were created using the English Web Treebank annotation guidelines (Bies et al., 2012), which are an addendum to the Penn Treebank guidelines (Bies et al., 1995).]
Sentence
   sofa: _InitialView
   begin: 2184
   end: 2388
[ These guidelines discuss domain-specific phenomena, including adaptations of existing labels as well as the addition of new labels for novel linguistic constructions.]
Sentence
   sofa: _InitialView
   begin: 2388
   end: 2555
[ Foster et al. (2011a) de- scribe a constituency treebank consisting of two domains; Twitter and sports forums.]
Sentence
   sofa: _InitialView
   begin: 2555
   end: 2666
[ The Twitter part is of compa- rable size to our treebank and is described in more detail in Foster et al. (2011b).]
Sentence
   sofa: _InitialView
   begin: 2666
   end: 2781
[ The dependency treebanks show greater diversity in annotation.]
Sentence
   sofa: _InitialView
   begin: 2781
   end: 2844
[ The English Web Treebank (Silveira et al., 2014) is annotated using the Universal Dependencies guidelines with additional relation types for the web domain.]
Sentence
   sofa: _InitialView
   begin: 2844
   end: 3001
[ A very different approach is taken for the annotation of the Tweebank (Kong et al., 2014).]
Sentence
   sofa: _InitialView
   begin: 3001
   end: 3092
[ different approach is taken for the annotation of the Tweebank (Kong et al., 2014). In its format, individual words can be skipped in the annotation. This is motivated by the idea that not all words in a Tweet contribute significantly to the syntactic structure and their inclusion would lead to arbitrary decisions unhelpful for most downstream applications. Additionally, because Tweets are used as units instead of sentences, having multiple roots is allowed. This adjusted dependency format makes it harder to use existing parsers with this dataset. The Foreebank (Kaljahi et al., 2015), a treebank focusing on forum text, is the only other treebank that includes normalization annotation. It includes manual normalizations of the raw text, and constituency trees of the normalized sentences. The normalization is kept as minimal as possible and is represented in the tree by appending an error suffix to the POS tags. The Foreebank allows analysis of the effect of different errors on the parsing performance of a constituency parser. Our contribution, the Denoised Web Treebank, fills the gap of a native (i.e., non-converted) dependency treebank including normalizations for the web domain. In the past, automatic conversions were used for this task (Petrov and McDonald, 2012; Foster et al., 2011a) using the Stanford Converter (De Marneffe et al., 2006). But for the noisy web domain, the conversions might be of questionable quality. Previous work on the parsing of web data has mostly focused on complementing existing parsers with semi- supervised data The amount of training data can be artifi- cially enlarged by using self-training or up-training (Petrov and McDonald, 2012; Foster et al., 2011b). Another source of semi-supervised improvements can be gained from using features gathered from large amounts of unannotated texts (Kong et al., 2014). A completely different approach is taken by Khan et al. (2013), where the most appropri- ate training trees are found in the train treebank for each sentence.]
Paragraph
   sofa: _InitialView
   begin: 3008
   end: 5031
[ In its format, individual words can be skipped in the annotation.]
Sentence
   sofa: _InitialView
   begin: 3092
   end: 3158
[ This is motivated by the idea that not all words in a Tweet contribute significantly to the syntactic structure and their inclusion would lead to arbitrary decisions unhelpful for most downstream applications.]
Sentence
   sofa: _InitialView
   begin: 3158
   end: 3368
[ Additionally, because Tweets are used as units instead of sentences, having multiple roots is allowed.]
Sentence
   sofa: _InitialView
   begin: 3368
   end: 3471
[ This adjusted dependency format makes it harder to use existing parsers with this dataset.]
Sentence
   sofa: _InitialView
   begin: 3471
   end: 3562
[ The Foreebank (Kaljahi et al., 2015), a treebank focusing on forum text, is the only other treebank that includes normalization annotation.]
Sentence
   sofa: _InitialView
   begin: 3562
   end: 3702
[ It includes manual normalizations of the raw text, and constituency trees of the normalized sentences.]
Sentence
   sofa: _InitialView
   begin: 3702
   end: 3805
[ The normalization is kept as minimal as possible and is represented in the tree by appending an error suffix to the POS tags.]
Sentence
   sofa: _InitialView
   begin: 3805
   end: 3931
[ The Foreebank allows analysis of the effect of different errors on the parsing performance of a constituency parser.]
Sentence
   sofa: _InitialView
   begin: 3931
   end: 4048
[ Our contribution, the Denoised Web Treebank, fills the gap of a native (i.e., non-converted) dependency treebank including normalizations for the web domain.]
Sentence
   sofa: _InitialView
   begin: 4048
   end: 4206
[ In the past, automatic conversions were used for this task (Petrov and McDonald, 2012; Foster et al., 2011a) using the Stanford Converter (De Marneffe et al., 2006).]
Sentence
   sofa: _InitialView
   begin: 4206
   end: 4372
[ But for the noisy web domain, the conversions might be of questionable quality.]
Sentence
   sofa: _InitialView
   begin: 4372
   end: 4452
[ Previous work on the parsing of web data has mostly focused on complementing existing parsers with semi- supervised data The amount of training data can be artifi- cially enlarged by using self-training or up-training (Petrov and McDonald, 2012; Foster et al., 2011b).]
Sentence
   sofa: _InitialView
   begin: 4452
   end: 4721
[ Another source of semi-supervised improvements can be gained from using features gathered from large amounts of unannotated texts (Kong et al., 2014).]
Sentence
   sofa: _InitialView
   begin: 4721
   end: 4872
[ A completely different approach is taken by Khan et al. (2013), where the most appropri- ate training trees are found in the train treebank for each sentence.]
Sentence
   sofa: _InitialView
   begin: 4872
   end: 5031
[The goal of the normalization was to leave the original tokens intact and not to replace them by their normalized forms directly. Hence, we keep both the original tokens and the normalized version of the sentences with word alignments. Figure 1 depicts a gold standard dependency graph including the alignments to the original tokens. Abbreviations Abbreviations and slang expressions are expanded whenever necessary for syntactic reasons. Exam- ples include instances such as “cu”, used as the short form of see and you, which as a single token would include both the verb and the object of the sentence. Punctuation Punctuation is inserted if it is necessary to disambiguate the sentence meaning. Emoticons, such as :), are kept intact. Zero copulas The data contains several cases of zero copula, i.e. a copula verb is not realized in the sentence. These occurrences are annotated by inserting the copula verb in the normalized version of the sentence (see Figure 1).]
Paragraph
   sofa: _InitialView
   begin: 5031
   end: 6001
[The goal of the normalization was to leave the original tokens intact and not to replace them by their normalized forms directly.]
Sentence
   sofa: _InitialView
   begin: 5031
   end: 5160
[ Hence, we keep both the original tokens and the normalized version of the sentences with word alignments.]
Sentence
   sofa: _InitialView
   begin: 5160
   end: 5266
[ Figure 1 depicts a gold standard dependency graph including the alignments to the original tokens.]
Sentence
   sofa: _InitialView
   begin: 5266
   end: 5365
[ Abbreviations Abbreviations and slang expressions are expanded whenever necessary for syntactic reasons.]
Sentence
   sofa: _InitialView
   begin: 5365
   end: 5470
[ Exam- ples include instances such as “cu”, used as the short form of see and you, which as a single token would include both the verb and the object of the sentence.]
Sentence
   sofa: _InitialView
   begin: 5470
   end: 5636
[ Punctuation Punctuation is inserted if it is necessary to disambiguate the sentence meaning.]
Sentence
   sofa: _InitialView
   begin: 5636
   end: 5729
[ Emoticons, such as :), are kept intact.]
Sentence
   sofa: _InitialView
   begin: 5729
   end: 5769
[ Zero copulas The data contains several cases of zero copula, i.e.]
Sentence
   sofa: _InitialView
   begin: 5769
   end: 5835
[ a copula verb is not realized in the sentence.]
Sentence
   sofa: _InitialView
   begin: 5835
   end: 5882
[ These occurrences are annotated by inserting the copula verb in the normalized version of the sentence (see Figure 1).]
Sentence
   sofa: _InitialView
   begin: 5882
   end: 6001
[The normalized tokens were automatically parsed using a generative phrase structure parser 2 and then converted to dependencies. Both part-of-speech tags and dependency annotations were then manually corrected in two passes. The dependency annotations follow the format of the CoNLL-X shared task on multilingual dependency parsing (Buchholz and Marsi, 2006), with the following adaptations: Emoticons Emoticons are kept intact, tagged with the part-of-speech tag UH (interjection) and are attached to the head of the sentence.]
Paragraph
   sofa: _InitialView
   begin: 6001
   end: 6528
[The normalized tokens were automatically parsed using a generative phrase structure parser 2 and then converted to dependencies.]
Sentence
   sofa: _InitialView
   begin: 6001
   end: 6129
[ Both part-of-speech tags and dependency annotations were then manually corrected in two passes.]
Sentence
   sofa: _InitialView
   begin: 6129
   end: 6225
[ The dependency annotations follow the format of the CoNLL-X shared task on multilingual dependency parsing (Buchholz and Marsi, 2006), with the following adaptations: Emoticons Emoticons are kept intact, tagged with the part-of-speech tag UH (interjection) and are attached to the head of the sentence.]
Sentence
   sofa: _InitialView
   begin: 6225
   end: 6528
[ Domain-specific elements Twitter-specific syntax was treated as follows: Usernames at the start of a sentence that do not fulfill a syntactic role, e.g. as the subject, are attached to the main verb using the DEP (unclassified) dependency relation. In all other cases, usernames are treated as proper nouns. RT and similar markers, as well as hashtags indicat- ing the topic of a Tweet (e.g. #worldcup) are attached to the main verb as DEP.]
Paragraph
   sofa: _InitialView
   begin: 6528
   end: 6969
[Domain-specific elements Twitter-specific syntax was treated as follows: Usernames at the start of a sentence that do not fulfill a syntactic role, e.g.]
Sentence
   sofa: _InitialView
   begin: 6529
   end: 6681
[ as the subject, are attached to the main verb using the DEP (unclassified) dependency relation.]
Sentence
   sofa: _InitialView
   begin: 6681
   end: 6777
[ In all other cases, usernames are treated as proper nouns.]
Sentence
   sofa: _InitialView
   begin: 6777
   end: 6836
[ RT and similar markers, as well as hashtags indicat- ing the topic of a Tweet (e.g.]
Sentence
   sofa: _InitialView
   begin: 6836
   end: 6920
[ #worldcup) are attached to the main verb as DEP.]
Sentence
   sofa: _InitialView
   begin: 6920
   end: 6969
[1 English Aspell dictionary: http://aspell.net/ 2 https://code.google.com/p/ berkeleyparser/]
Paragraph
   sofa: _InitialView
   begin: 6969
   end: 7061
[P ROOT COORD P OBJ NMOD SBJ NMOD OBJ You . are . the . little . . guy . , . are . you . ? . U ∅ da lil guy , ru ?]
Paragraph
   sofa: _InitialView
   begin: 7061
   end: 7174
[We collected all Tweets within a period of 24 hours from January 07, 2012 00:00 until 23:59 GMT. To avoid possible]
Paragraph
   sofa: _InitialView
   begin: 7174
   end: 7288
[We collected all Tweets within a period of 24 hours from January 07, 2012 00:00 until 23:59 GMT.]
Sentence
   sofa: _InitialView
   begin: 7174
   end: 7270
[]
Sentence
   sofa: _InitialView
   begin: 7174
   end: 7174
[ To avoid possible biases of automatic language identification tools towards well-formed language, we manually classified the Tweets in random order into English and non-English Tweets until we reached a reasonably-sized corpus of Tweets classified as English.]
Sentence
   sofa: _InitialView
   begin: 7270
   end: 7530
[ biases of automatic language identification tools towards well-formed language, we manually classified the Tweets in random order into English and non-English Tweets until we reached a reasonably-sized corpus of Tweets classified as English. We then manually split this corpus into sentences and randomly selected 250 sentences as a development set and 250 sentences as a test set. Table 1 compares some basic statistics of this treebank against other Web treebanks. Out-of-vocabulary rate is calculated against the English dictionary of the GNU Aspell spell checker. 1]
Paragraph
   sofa: _InitialView
   begin: 7288
   end: 7858
[ We then manually split this corpus into sentences and randomly selected 250 sentences as a development set and 250 sentences as a test set.]
Sentence
   sofa: _InitialView
   begin: 7530
   end: 7670
[ Table 1 compares some basic statistics of this treebank against other Web treebanks.]
Sentence
   sofa: _InitialView
   begin: 7670
   end: 7755
[ Out-of-vocabulary rate is calculated against the English dictionary of the GNU Aspell spell checker.]
Sentence
   sofa: _InitialView
   begin: 7755
   end: 7856
[ 1]
Sentence
   sofa: _InitialView
   begin: 7856
   end: 7858
[Our dataset provides alignments between the gold standard and the original tokens, allowing for insertions, deletions and modifications. Hence, the standard dependency parsing metrics, unlabeled and labeled attachment scores, are no longer sufficient. In our dataset, there may not be a di- rect one-to-one correspondence between the predicted tree and the gold tree. Hence, we allow the parser to make any insertions, deletions and modifications to the tokens under the assumption that it provides an alignment between the modified tokens and the original tokens. The evaluation is then performed using a metric based on precision and recall values calculated using these alignments.]
Paragraph
   sofa: _InitialView
   begin: 7858
   end: 8542
[Our dataset provides alignments between the gold standard and the original tokens, allowing for insertions, deletions and modifications.]
Sentence
   sofa: _InitialView
   begin: 7858
   end: 7994
[]
Sentence
   sofa: _InitialView
   begin: 7858
   end: 7858
[ Hence, the standard dependency parsing metrics, unlabeled and labeled attachment scores, are no longer sufficient.]
Sentence
   sofa: _InitialView
   begin: 7994
   end: 8109
[ In our dataset, there may not be a di- rect one-to-one correspondence between the predicted tree and the gold tree.]
Sentence
   sofa: _InitialView
   begin: 8109
   end: 8225
[ Hence, we allow the parser to make any insertions, deletions and modifications to the tokens under the assumption that it provides an alignment between the modified tokens and the original tokens.]
Sentence
   sofa: _InitialView
   begin: 8225
   end: 8422
[ The evaluation is then performed using a metric based on precision and recall values calculated using these alignments.]
Sentence
   sofa: _InitialView
   begin: 8422
   end: 8542
[ Aligned precision and recall Based on the normalized side of the gold standard and the parser’s aligned predictions, we calculate precision, recall and F 1 score for dependencies (Eq. 1–3). We base the evaluation metric on the standard definitions of precision and recall, which are widely used in natural language processing. In Eq. 1 and 2, TP , FP and FN are the numbers of true positive, false positive, and false negative results. The F 1 measure is the harmonic mean of precision and recall.]
Paragraph
   sofa: _InitialView
   begin: 8542
   end: 9040
[Aligned precision and recall Based on the normalized side of the gold standard and the parser’s aligned predictions, we calculate precision, recall and F 1 score for dependencies (Eq.]
Sentence
   sofa: _InitialView
   begin: 8543
   end: 8726
[ 1–3).]
Sentence
   sofa: _InitialView
   begin: 8726
   end: 8732
[ We base the evaluation metric on the standard definitions of precision and recall, which are widely used in natural language processing.]
Sentence
   sofa: _InitialView
   begin: 8732
   end: 8869
[ In Eq.]
Sentence
   sofa: _InitialView
   begin: 8869
   end: 8876
[ 1 and 2, TP , FP and FN are the numbers of true positive, false positive, and false negative results.]
Sentence
   sofa: _InitialView
   begin: 8876
   end: 8978
[ The F 1 measure is the harmonic mean of precision and recall.]
Sentence
   sofa: _InitialView
   begin: 8978
   end: 9040
[TP precision = (1) TP + FP TP recall = (2) TP + FN precision · recall F 1 = 2 · (3) precision + recall]
Paragraph
   sofa: _InitialView
   begin: 9040
   end: 9142
[TP precision = (1) TP + FP TP recall = (2) TP + FN precision · recall F 1 = 2 · (3) precision + recall]
Sentence
   sofa: _InitialView
   begin: 9040
   end: 9142
[Definition Formally, when comparing the parser output against the dataset, the following information is provided for each instance: − the original sentence S O − a predicted dependency tree D P = ⟨V P , E P ⟩ − a gold dependency tree D G = ⟨V G , E G ⟩ − alignment function a P for predicted tokens − alignment function a G for gold tokens For each parsed dependency tree, S O is the sequence of original, non-normalized tokens. The two alignment functions a G and a P map the gold tokens and the predicted tokens to the original tokens in S O . In the case of an insertion, the new token cannot be aligned to any of the original tokens in S O and, therefore, such insertions are mapped to an artificial NULL token. Unlabeled dependencies Based on all test instances, we calculate the total number of true positive, false positive and false negative dependency relations as follows: For each gold dependency tree D G = ⟨V G , E G ⟩ and each predicted dependency tree D P = ⟨V P , E P ⟩, let M G and M P be the set of dependency relations mapped to the original tokens in S O : M G = {⟨a G (w i ), a G (w j )⟩ | ⟨w i , r, w j ⟩ ∈ E G } M P = {⟨a P (w i ), a P (w j )⟩ | ⟨w i , r, w j ⟩ ∈ E P } The true positive, false positive and false negative dependency relations can then be calculated as: ∑ TP = |M G ∩ M P | ⟨S O ,D P ,D G ,a P ,a G ⟩ ∑ FP = |M P \ M G | ⟨S O ,D P ,D G ,a P ,a G ⟩ ∑ FN = |M G \ M P | ⟨S O ,D P ,D G ,a P ,a G ⟩ Labeled dependencies To measure labeled dependencies, the dependency type is added to the head-modifier pair in M P and M G : M G ′ = {⟨a G (w i ), r, a G (w j )⟩ | ⟨w i , r, w j ⟩ ∈ E G } M P ′ = {⟨a P (w i ), r, a P (w j )⟩ | ⟨w i , r, w j ⟩ ∈ E P } Relation to other metrics This metric can be seen as a generalization of the commonly used attachment score measure. If there is a one-to-one alignment between every predicted token and every gold token, the unlabeled and labeled aligned F 1 scores are equivalent to the unlabeled (UAS) and labeled attachment score (LAS).]
Paragraph
   sofa: _InitialView
   begin: 9142
   end: 11151
[Definition Formally, when comparing the parser output against the dataset, the following information is provided for each instance: − the original sentence S O − a predicted dependency tree D P = ⟨V P , E P ⟩ − a gold dependency tree D G = ⟨V G , E G ⟩ − alignment function a P for predicted tokens − alignment function a G for gold tokens For each parsed dependency tree, S O is the sequence of original, non-normalized tokens.]
Sentence
   sofa: _InitialView
   begin: 9142
   end: 9570
[ The two alignment functions a G and a P map the gold tokens and the predicted tokens to the original tokens in S O .]
Sentence
   sofa: _InitialView
   begin: 9570
   end: 9687
[ In the case of an insertion, the new token cannot be aligned to any of the original tokens in S O and, therefore, such insertions are mapped to an artificial NULL token.]
Sentence
   sofa: _InitialView
   begin: 9687
   end: 9857
[ Unlabeled dependencies Based on all test instances, we calculate the total number of true positive, false positive and false negative dependency relations as follows: For each gold dependency tree D G = ⟨V G , E G ⟩ and each predicted dependency tree D P = ⟨V P , E P ⟩, let M G and M P be the set of dependency relations mapped to the original tokens in S O : M G = {⟨a G (w i ), a G (w j )⟩ | ⟨w i , r, w j ⟩ ∈ E G } M P = {⟨a P (w i ), a P (w j )⟩ | ⟨w i , r, w j ⟩ ∈ E P } The true positive, false positive and false negative dependency relations can then be calculated as: ∑ TP = |M G ∩ M P | ⟨S O ,D P ,D G ,a P ,a G ⟩ ∑ FP = |M P \ M G | ⟨S O ,D P ,D G ,a P ,a G ⟩ ∑ FN = |M G \ M P | ⟨S O ,D P ,D G ,a P ,a G ⟩ Labeled dependencies To measure labeled dependencies, the dependency type is added to the head-modifier pair in M P and M G : M G ′ = {⟨a G (w i ), r, a G (w j )⟩ | ⟨w i , r, w j ⟩ ∈ E G } M P ′ = {⟨a P (w i ), r, a P (w j )⟩ | ⟨w i , r, w j ⟩ ∈ E P } Relation to other metrics This metric can be seen as a generalization of the commonly used attachment score measure.]
Sentence
   sofa: _InitialView
   begin: 9857
   end: 10945
[ If there is a one-to-one alignment between every predicted token and every gold token, the unlabeled and labeled aligned F 1 scores are equivalent to the unlabeled (UAS) and labeled attachment score (LAS).]
Sentence
   sofa: _InitialView
   begin: 10945
   end: 11151
[Having introduced our dataset and the corresponding evaluation metric, we can evaluate the impact of two methods commonly used to aid in the parsing of noisy content: noise-robust part-of-speech tagging and text normalization.]
Paragraph
   sofa: _InitialView
   begin: 11151
   end: 11377
[Having introduced our dataset and the corresponding evaluation metric, we can evaluate the impact of two methods commonly used to aid in the parsing of noisy content: noise-robust part-of-speech tagging and text normalization.]
Sentence
   sofa: _InitialView
   begin: 11151
   end: 11377
[POS tagging is a necessary preprocessing step for many parsing algorithms. Previous studies (e.g., Foster et al. (2011b)) have shown that the accuracy of POS tagging can suffer significantly from noisy content. However, it is possible to adapt POS taggers to this type of input. In this ex- periment, we will briefly introduce approaches to adapting POS taggers and perform an evaluation on our dataset. Domain-specific tagging Gimpel et al. (2011) present a domain-specific conditional random field POS tagger using a coarse part-of-speech tagset of 25 tags that was specifically designed for and trained on Twitter data. The tagset includes tags for social media-specific tokens, such as URLs, email addresses, emoticons, Twitter hashtags and usernames. Role of POS tags in the parser For our experiments, we use the discriminative graph-based maximum spanning tree (MST) parser (McDonald et al., 2005). This dependency parser expects both fine- and coarse-grained tags as features in its well-established standard setting. Since we are interested in the influence of POS tagging on parse quality instead of the impact of individual features in the parser, we use this standard setting but combine the coarse- grained tags from the domain-specific tagger with the POS tags produced by a POS tagger with a less coarse-grained tagset. Both are combined by first determining n-best fine- grained tags for each token. For hidden Markov models, the probability of the tags occurring at a given position can be calculated using the forward-backward algorithm as P (t i = t) = α i (t)β i (t), where α i (t) is the total probability of all possible tag sequences ending in the tag t at the ith token and β i (t) is the total probability of all tag sequences starting from tag t at the ith token and contin- uing to the end of the sentence (Jurafsky and Martin, 2000; Prins, 2005). The n-best fine-grained tags are then combined with the coarse tags by a simple voting rule. Our experiments use a standard trigram HMM tagger 3 (Brants, 2000) and the OpenNLP maximum entropy tagger. 4 Impact on parse quality Table 2 shows the influence of POS tagging on the performance of the MST parser on the development part of our dataset. Statistical significance testing is performed using bootstrap resampling (Efron and Tibshirani, 1993). Except for the last row of the table, all tagging is performed without any text normalization. The last row demonstrates the upper bound performance on this task, by using both gold text normalization and gold part-of-speech tags. These results show that combining a generic part-of-speech tagger with a more coarse-grained domain-specific tagger can lead to measurable improvements in parse quality.]
Paragraph
   sofa: _InitialView
   begin: 11377
   end: 14101
[POS tagging is a necessary preprocessing step for many parsing algorithms.]
Sentence
   sofa: _InitialView
   begin: 11377
   end: 11451
[ Previous studies (e.g., Foster et al. (2011b)) have shown that the accuracy of POS tagging can suffer significantly from noisy content.]
Sentence
   sofa: _InitialView
   begin: 11451
   end: 11587
[ However, it is possible to adapt POS taggers to this type of input.]
Sentence
   sofa: _InitialView
   begin: 11587
   end: 11655
[ In this ex- periment, we will briefly introduce approaches to adapting POS taggers and perform an evaluation on our dataset.]
Sentence
   sofa: _InitialView
   begin: 11655
   end: 11780
[ Domain-specific tagging Gimpel et al. (2011) present a domain-specific conditional random field POS tagger using a coarse part-of-speech tagset of 25 tags that was specifically designed for and trained on Twitter data.]
Sentence
   sofa: _InitialView
   begin: 11780
   end: 11999
[ The tagset includes tags for social media-specific tokens, such as URLs, email addresses, emoticons, Twitter hashtags and usernames.]
Sentence
   sofa: _InitialView
   begin: 11999
   end: 12132
[ Role of POS tags in the parser For our experiments, we use the discriminative graph-based maximum spanning tree (MST) parser (McDonald et al., 2005).]
Sentence
   sofa: _InitialView
   begin: 12132
   end: 12282
[ This dependency parser expects both fine- and coarse-grained tags as features in its well-established standard setting.]
Sentence
   sofa: _InitialView
   begin: 12282
   end: 12402
[ Since we are interested in the influence of POS tagging on parse quality instead of the impact of individual features in the parser, we use this standard setting but combine the coarse- grained tags from the domain-specific tagger with the POS tags produced by a POS tagger with a less coarse-grained tagset.]
Sentence
   sofa: _InitialView
   begin: 12402
   end: 12711
[ Both are combined by first determining n-best fine- grained tags for each token.]
Sentence
   sofa: _InitialView
   begin: 12711
   end: 12792
[ For hidden Markov models, the probability of the tags occurring at a given position can be calculated using the forward-backward algorithm as P (t i = t) = α i (t)β i (t), where α i (t) is the total probability of all possible tag sequences ending in the tag t at the ith token and β i (t) is the total probability of all tag sequences starting from tag t at the ith token and contin- uing to the end of the sentence (Jurafsky and Martin, 2000; Prins, 2005).]
Sentence
   sofa: _InitialView
   begin: 12792
   end: 13251
[ The n-best fine-grained tags are then combined with the coarse tags by a simple voting rule.]
Sentence
   sofa: _InitialView
   begin: 13251
   end: 13344
[ Our experiments use a standard trigram HMM tagger 3 (Brants, 2000) and the OpenNLP maximum entropy tagger.]
Sentence
   sofa: _InitialView
   begin: 13344
   end: 13451
[ 4 Impact on parse quality Table 2 shows the influence of POS tagging on the performance of the MST parser on the development part of our dataset.]
Sentence
   sofa: _InitialView
   begin: 13451
   end: 13597
[ Statistical significance testing is performed using bootstrap resampling (Efron and Tibshirani, 1993).]
Sentence
   sofa: _InitialView
   begin: 13597
   end: 13700
[ Except for the last row of the table, all tagging is performed without any text normalization.]
Sentence
   sofa: _InitialView
   begin: 13700
   end: 13795
[ The last row demonstrates the upper bound performance on this task, by using both gold text normalization and gold part-of-speech tags.]
Sentence
   sofa: _InitialView
   begin: 13795
   end: 13931
[ These results show that combining a generic part-of-speech tagger with a more coarse-grained domain-specific tagger can lead to measurable improvements in parse quality.]
Sentence
   sofa: _InitialView
   begin: 13931
   end: 14101
[3 https://github.com/danieldk/jitar 4 http://opennlp.apache.org/]
Paragraph
   sofa: _InitialView
   begin: 14101
   end: 14165
[After considering the influence of the underlying POS tagger on parse quality, we now turn to the question of how much the parsing of noisy content is influenced by text normalization. For this, we evaluate two common text normalization methods: unsupervised normalization via lexical replacements and normalization based on machine translation. Unsupervised lexical normalization Various unsupervised methods for text normalization have been suggested in the relevant literature. A popular approach is to perform lexical normalization by correcting individual tokens. We implement the model for lexical normalization of text messages by Han and Baldwin (2011). This method works in analogy to spell checking, with the biggest difference that in short message data ill-formedness is often intentional, for example due to the message size limit. The model performs normalization only on the token level. Normalization as machine translation Research in short message normalization has shown that another effec- tive method is to treat the task as a machine translation problem. Aw et al. (2006) and Raghunathan and Krawczyk (2009) explore phrase-based statistical machine translation as a preprocessing step for various NLP tasks involving text messages. As part of this effort, they manually normalize a set of 5.000 and 2.500 messages respectively. While these corpora are not created for social media services such as Twitter, they nonetheless provide reasonable training corpora for our experiments as the restrictions of both domains are similar. Based on this corpus, we train a standard Moses baseline system 5 (Koehn et al., 2007) using GIZA++ for word alignments and the grow-diag-final symmetrization heuristic. An n-gram language model is built on the English side of the news-commentary data set using IRSTLM (Federico and Cettolo, 2007). Model weights are estimated using MERT (Och, 2003). All experiments are performed on the development part of our dataset. Twitter-specific processing In order to isolate the influence of the text normalization, Twitter-specific syntax is parsed using a set of deterministic rules. Tokens such as retweet indicators and usernames at the start of a Tweet and URLs and hash tags at the end of a Tweet are removed from the text and pushed onto a stack. The remaining text is]
Paragraph
   sofa: _InitialView
   begin: 14165
   end: 16485
[After considering the influence of the underlying POS tagger on parse quality, we now turn to the question of how much the parsing of noisy content is influenced by text normalization.]
Sentence
   sofa: _InitialView
   begin: 14165
   end: 14349
[]
Sentence
   sofa: _InitialView
   begin: 14165
   end: 14165
[ For this, we evaluate two common text normalization methods: unsupervised normalization via lexical replacements and normalization based on machine translation.]
Sentence
   sofa: _InitialView
   begin: 14349
   end: 14510
[ Unsupervised lexical normalization Various unsupervised methods for text normalization have been suggested in the relevant literature.]
Sentence
   sofa: _InitialView
   begin: 14510
   end: 14645
[ A popular approach is to perform lexical normalization by correcting individual tokens.]
Sentence
   sofa: _InitialView
   begin: 14645
   end: 14733
[ We implement the model for lexical normalization of text messages by Han and Baldwin (2011).]
Sentence
   sofa: _InitialView
   begin: 14733
   end: 14826
[ This method works in analogy to spell checking, with the biggest difference that in short message data ill-formedness is often intentional, for example due to the message size limit.]
Sentence
   sofa: _InitialView
   begin: 14826
   end: 15009
[ The model performs normalization only on the token level.]
Sentence
   sofa: _InitialView
   begin: 15009
   end: 15067
[ Normalization as machine translation Research in short message normalization has shown that another effec- tive method is to treat the task as a machine translation problem.]
Sentence
   sofa: _InitialView
   begin: 15067
   end: 15241
[ Aw et al. (2006) and Raghunathan and Krawczyk (2009) explore phrase-based statistical machine translation as a preprocessing step for various NLP tasks involving text messages.]
Sentence
   sofa: _InitialView
   begin: 15241
   end: 15418
[ As part of this effort, they manually normalize a set of 5.000 and 2.500 messages respectively.]
Sentence
   sofa: _InitialView
   begin: 15418
   end: 15514
[ While these corpora are not created for social media services such as Twitter, they nonetheless provide reasonable training corpora for our experiments as the restrictions of both domains are similar.]
Sentence
   sofa: _InitialView
   begin: 15514
   end: 15715
[ Based on this corpus, we train a standard Moses baseline system 5 (Koehn et al., 2007) using GIZA++ for word alignments and the grow-diag-final symmetrization heuristic.]
Sentence
   sofa: _InitialView
   begin: 15715
   end: 15885
[ An n-gram language model is built on the English side of the news-commentary data set using IRSTLM (Federico and Cettolo, 2007).]
Sentence
   sofa: _InitialView
   begin: 15885
   end: 16014
[ Model weights are estimated using MERT (Och, 2003).]
Sentence
   sofa: _InitialView
   begin: 16014
   end: 16066
[ All experiments are performed on the development part of our dataset.]
Sentence
   sofa: _InitialView
   begin: 16066
   end: 16136
[ Twitter-specific processing In order to isolate the influence of the text normalization, Twitter-specific syntax is parsed using a set of deterministic rules.]
Sentence
   sofa: _InitialView
   begin: 16136
   end: 16295
[ Tokens such as retweet indicators and usernames at the start of a Tweet and URLs and hash tags at the end of a Tweet are removed from the text and pushed onto a stack.]
Sentence
   sofa: _InitialView
   begin: 16295
   end: 16463
[ The remaining text is then parsed using the underlying dependency parser and the Twitter-specific tokens are re-attached to the tree accord- ing to a fixed set of rules.]
Sentence
   sofa: _InitialView
   begin: 16463
   end: 16633
[ then parsed using the underlying dependency parser and the Twitter-specific tokens are re-attached to the tree accord- ing to a fixed set of rules. This deterministic handling of Twitter-specific syntax is applied to all further experiments in Table 3. Impact on parse quality Table 3 presents the results of the text normalization schemes on the development part of our dataset. The results show that a combination of lexical and MT-based normalization approaches leads to results close to the upper bound set by gold standard normalization. Although the machine translation system was trained on a different domain, its application leads to better parsing results. This improved performance is most likely due to the fact that the method is able to normalize sequences of words on the phrase level instead of being restricted to single-word replacements.]
Paragraph
   sofa: _InitialView
   begin: 16485
   end: 17342
[ This deterministic handling of Twitter-specific syntax is applied to all further experiments in Table 3.]
Sentence
   sofa: _InitialView
   begin: 16633
   end: 16738
[ Impact on parse quality Table 3 presents the results of the text normalization schemes on the development part of our dataset.]
Sentence
   sofa: _InitialView
   begin: 16738
   end: 16865
[ The results show that a combination of lexical and MT-based normalization approaches leads to results close to the upper bound set by gold standard normalization.]
Sentence
   sofa: _InitialView
   begin: 16865
   end: 17028
[ Although the machine translation system was trained on a different domain, its application leads to better parsing results.]
Sentence
   sofa: _InitialView
   begin: 17028
   end: 17152
[ This improved performance is most likely due to the fact that the method is able to normalize sequences of words on the phrase level instead of being restricted to single-word replacements.]
Sentence
   sofa: _InitialView
   begin: 17152
   end: 17342
[5 http://statmt.org/moses/?n=Moses. Baseline]
Paragraph
   sofa: _InitialView
   begin: 17342
   end: 17386
[* statistically significant against non-normalized baseline at p-value < 0.05.]
Paragraph
   sofa: _InitialView
   begin: 17386
   end: 17464
[User-generated content on the web constitutes a rich and important source of information for many use cases. However, parsing of such noisy data still poses challenges for many parsing algorithms. In this paper, we have compared various strategies for adapting dependency parsing to noisy input conditions. In order to do so, we introduced a noise- aware benchmark for dependency parsing consisting of a treebank and a corresponding evaluation metric. Our experiments on this new dataset show that text normalization improves parse quality significantly, especially if the normalization method can go beyond the word level (e.g. using machine translation). To encourage future progress in this area, we make available both the Denoised Web Treebank and the newly introduced noise-aware evaluation metric. 7]
Paragraph
   sofa: _InitialView
   begin: 17464
   end: 18270
[User-generated content on the web constitutes a rich and important source of information for many use cases.]
Sentence
   sofa: _InitialView
   begin: 17464
   end: 17572
[]
Sentence
   sofa: _InitialView
   begin: 17464
   end: 17464
[ However, parsing of such noisy data still poses challenges for many parsing algorithms.]
Sentence
   sofa: _InitialView
   begin: 17572
   end: 17660
[ In this paper, we have compared various strategies for adapting dependency parsing to noisy input conditions.]
Sentence
   sofa: _InitialView
   begin: 17660
   end: 17770
[ In order to do so, we introduced a noise- aware benchmark for dependency parsing consisting of a treebank and a corresponding evaluation metric.]
Sentence
   sofa: _InitialView
   begin: 17770
   end: 17915
[ Our experiments on this new dataset show that text normalization improves parse quality significantly, especially if the normalization method can go beyond the word level (e.g.]
Sentence
   sofa: _InitialView
   begin: 17915
   end: 18092
[ using machine translation).]
Sentence
   sofa: _InitialView
   begin: 18092
   end: 18120
[ To encourage future progress in this area, we make available both the Denoised Web Treebank and the newly introduced noise-aware evaluation metric.]
Sentence
   sofa: _InitialView
   begin: 18120
   end: 18268
[ 7]
Sentence
   sofa: _InitialView
   begin: 18268
   end: 18270
[We thank Gertjan van Noord for his valuable feedback. Parts of this work were supported through the Erasmus Mundus European Masters Program in Language and Com- munication Technologies (EM-LCT). The first author is supported by the EXPERT (EXPloiting Empirical ap- pRoaches to Translation) Initial Training Network (ITN) of the European Union’s Seventh Framework Programme. The second author is supported by the Nuance Foundation.]
Paragraph
   sofa: _InitialView
   begin: 18270
   end: 18700
[We thank Gertjan van Noord for his valuable feedback.]
Sentence
   sofa: _InitialView
   begin: 18270
   end: 18323
[ Parts of this work were supported through the Erasmus Mundus European Masters Program in Language and Com- munication Technologies (EM-LCT).]
Sentence
   sofa: _InitialView
   begin: 18323
   end: 18464
[ The first author is supported by the EXPERT (EXPloiting Empirical ap- pRoaches to Translation) Initial Training Network (ITN) of the European Union’s Seventh Framework Programme.]
Sentence
   sofa: _InitialView
   begin: 18464
   end: 18643
[ The second author is supported by the Nuance Foundation.]
Sentence
   sofa: _InitialView
   begin: 18643
   end: 18700
[6 Tags predicted by coarse + n-best MaxEnt. 7 http://jodaiber.de/DenoisedWebTreebank]
Paragraph
   sofa: _InitialView
   begin: 18700
   end: 18784
-------- View _InitialView end ----------------------------------

======== CAS 0 end ==================================


